"""Распознавание рукописных цифр MNIST."""
# На вход нейросети будет приходить картинка с изображением цифры, а на выходе вы получите значение цифры

"""1. Подготовка данных."""
import mnist                                            # Библиотека с базой рукописных цифр
from tensorflow.python.keras import utils               # Утилиты для подготовки данных  
from tensorflow.python.keras.models import Sequential   # Подключение класса создания модели Sequential
from tensorflow.python.keras.layers import Dense        # Подключение класса Dense - полносвязный слой           
import numpy as np                                      # Работа с массивами
import matplotlib.pyplot as plt                         # Отрисовка изображений


# Загрузка из облака данных Mnist

# x_train_org, y_train_org – изображения для обучения нейронной сети;
# x_test_org, y_test_org – изображения для тестирования нейронной сети.
(x_train_org, y_train_org), (x_test_org, y_test_org) = mnist.load_data()

# Сейчас картинки представляют из себя матрицы чисел, где каждое число - значение яркости пикселя (от 0 до 255)

# Вывод формы данных для обучения
x_train_org.shape # (60000, 28, 28)

# Отобразить картинку:
# Номер картинки
n = 143
# Отрисовка картинки:
plt.imshow(x_train_org[n], cmap='gray')
# Вывод картинки
plt.show()


# Вывод метки класса для n-го изображения
print(y_train_org[n]) # 2


# Сейчас данные имеют сложную структуру, где каждая картинка представляет собой двумерный массив данных.
# Для обучения нейронной сети необходимо преобразовать изображение в более простой вид – в одномерную последовательность чисел (вектор)

# Сделайте это с помощью метода .reshape():

# Изменение формы входных картинок с 28х28 на 784
# первая ось остается без изменения, остальные складываются в вектор
# .shape - узнать размеры массива по каждой из осей
# .reshape(x,y) - преобразовать массив в размер x*y. x = 60000, y = -1

x_train = x_train_org.reshape(x_train_org.shape[0], -1)
x_test = x_test_org.reshape(x_test_org.shape[0], -1)

# Проверка результата
print(f'Форма обучающих данных: {x_train_org.shape} -> {x_train.shape}')
print(f'Форма  тестовых данных: {x_test_org.shape} -> {x_test.shape}')


# Нормализация входных картинок (Сейчас от 0 до 255, а должны быть от 0 до 1)
# Преобразование x_train в тип float32 (числа с плавающей точкой) и нормализация
x_train = x_train.astype('float32') / 255.

# Преобразование x_test в тип float32 (числа с плавающей точкой) и нормализация
x_test = x_test.astype('float32') / 255.


# Также нужно преобразовать метки (ответы) в один длинный массив
# Для этого нужно привести все метки к виду one hot encoding.
# Например если ответ 5, то:
# [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]

# Задание константы количества распознаваемых классов. Т.к в ответе будет всего одна цифра от 1 до 10. Какая цифра изображена на картинке
CLASS_COUNT = 10
# Преобразуйте выходные данные в векторы one hot encoding с помощью функции to_categorical() модуля utils:
y_train = utils.to_categorical(y_train_org, CLASS_COUNT)
y_test = utils.to_categorical(y_test_org, CLASS_COUNT)

# Теперь метки будут выглядеть так: (60000 меток, от 1 до 10)
# Вывод формы y_train
# 60 тысяч примеров, каждый длины 10 по числу классов
print(y_train.shape)


# Вывод примера одного выходного вектора
print(y_train[0])                                   # [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
# Вывод метки, соответствующей 36-му элементу
print(y_train_org[36])                              # 6



"""2. Создание нейронной сети."""
# Создание последовательной модели
model = Sequential()

# Добавление полносвязного слоя на 800 нейронов с relu-активацией
model.add(Dense(800, input_dim=784, activation='relu'))
# Добавление полносвязного слоя на 400 нейронов с relu-активацией
model.add(Dense(400, activation='relu'))
# Добавление полносвязного слоя с количеством нейронов по числу классов с softmax-активацией
model.add(Dense(CLASS_COUNT, activation='softmax')) 

# 800 - кол-во нейронов в слое
# 'relu' - функция активации, которая будет применяться после умножения значений входов нейрона на его веса

# Функции активации - Определяют выходное значение нейрона в зависимости от результата взвешенной суммы входов и порогового значения.
# Определяет, передавать ли полученное значение следующему нейрону (активировать его) или нет, и модифицировать ли как-то полученное значение. ВИДЫ:

# Ступенчатая (binary step function) - если z больше или меньше некоторого значения, то нейрон становится активированным
# Линейная (linear) - Невозможность использования метода обратного распространения ошибки. Так как в основе этого метода обучения лежит градиентный спуск. Нет смысла создавать многослойную архитектуру. Плюс в том, что область определения не ограничена: (−∞;+∞) Следовательно, ее нужно использовать, когда выходное значение нейрона должно ∈R, а не ограниченному интервалу. инейная функция активации linear используется, например, для задач регрессии
# Сигмоидная (sigmoid) - Преобразует выходное значение нейрона в значение из диапазона [0, +1] и используется в случаях, когда нужно получить что-то похожее на вероятность. Хорошо себя показывает в LSTM-сетках, логистической регрессии. можно использовать в нейронных сетях с множеством слоев, а также обучать эти сети методом обратного распространения ошибки.
# Гиперболического тангенса (tahn) - то же самое что и sigmoid, но в диапазоне [-1; 1]. Используются в рамках моделей LSTM (Long Short Term Memory)
# ReLU (Rectified Linear Unit)  - возвращает входные значения без изменения, если знак значений положительный; отрицательные значения преобразует к нулю. Используется в сетях с множеством слоев. ReLU даёт хорошие результаты точности в полносвязных слоях (Dense layers) и свёрточных слоях (Convolutional layers)
# Softmax - распределяет выходные значения пропорционально вероятности каждого из них в диапазоне ( 0;1 ); это означает, что сумма выходных значений всегда будет равна 1 (что эквивалентно 100% общей вероятности); данная функция хорошо подходит для задач по многоклассовой классификации.


# Нейронная сеть создана. теперь нужно подготовить ее к обучению (скомпилировать) и запустить само обучение
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# model.compile(функция ошибок, оптимизатор и метрики, которые будут подсчитываться в процессе обучения)

# Вывод структуры модели
print(model.summary())

# Функция plot_model() модуля utils нарисует наглядную схему (граф) нейронной сети, она удобна для понимания и более сложных моделей.
utils.plot_model(
    model,                                  # Модель, схему которой необходимо построить (обяз)
    to_file='model_plot.png',               # Имя файла или путь к файлу, в который сохраняется схема (обяз)
    show_shapes=True,                       # Показывать или нет формы входных/выходных данных каждого слоя (необяз, дефолт=false)
    show_layer_names=False                  # show_layer_names - показывать или нет название каждого слоя (необяз, дефолт=true)
)


"""Обучение нейронной сети."""
# Обучение нейронной сети метод .fit()
model.fit(x_train,        # обучающая выборка, входные данные
          y_train,        # обучающая выборка, выходные данные
          batch_size=128, # кол-во примеров, которое обрабатывает нейронка перед одним изменением весов
          epochs=15,      # количество эпох, когда нейронка обучается на всех примерах выборки
          verbose=1)      # 0 - не визуализировать ход обучения, 1 - визуализировать

# Теперь сохраните веса вашей модели, чтобы потом можно было снова их использовать:
# Метод .save_weights() сохранит веса вашей модели в хранилище, а метод .load_weights() загрузит их обратно.
model.save_weights('model.h5')
model.load_weights('model.h5')


"""Распознавание рукописных цифр."""
# Номер тестовой цифры, которую будем распознавать
n_rec = np.random.randint(x_test_org.shape[0])
# Отображение картинки из тестового набора под номером n_rec
plt.imshow(x_test_org[n_rec], cmap='gray')
plt.show()


# Теперь сохраните в переменную x эту картинку в виде набора из 784 чисел.
# Выбор нужной картинки из тестовой выборки
x = x_test[n_rec]

# Проверка формы данных
print(x.shape)

# Добавление одной оси в начале, чтобы нейронка могла распознать пример
# Массив из одного примера, так как нейронка принимает именно массивы примеров (батчи) для распознавания
x = np.expand_dims(x, axis=0)

# Проверка формы данных
print(x.shape)



# Чтобы ваша сеть сделала предсказание, нужно вызвать метод .predict() и передать в него данные для распознавания:
# Распознавание примера
prediction = model.predict(x)

# Вывод результата - вектор из 10 чисел
print(prediction)
# [[2.1082565e-22 1.5469933e-12 8.0784421e-15 1.0000000e+00 5.8189000e-17
#   3.4360931e-11 1.8311961e-22 1.5410466e-13 1.9334787e-15 8.1246057e-17]]

# Эти числа характеризуют вероятности принадлежности к конкретному классу. Самое первое число в этой последовательности отвечает на вопрос, какова вероятность, что на картинке изображена цифра 0. Второе число говорит то же самое про цифру 1. Сумма всех вероятностей равна единице, то есть предполагатся полный набор событий (только цифры от 0 до 9 и ничего другого):


sum(prediction[0]) # 1.0000000005089957


# Получение и вывод индекса самого большого элемента (это значение цифры, которую распознала сеть)
pred = np.argmax(prediction)
print(f'Распознана цифра: {pred}')


# Вывод правильного ответа для сравнения
print(y_test_org[n_rec])





"""В первой части вы познакомились с ИИ, узнали, что он бывает сильным и слабым, и то, что к 2050 развитие технологии достигнет пика.
Для ИИ практически нет неразрешимых задач, и область его использования безгранична.
Еще вы узнали, что нейронная сеть неспроста так называется, и у нее есть прототип – человеческий мозг и нейроны, из которых он состоит.

Далее вы перешли к рассмотрению различных моделей нейрона (биологической и математической).
В НС нейроны отвечают за выделение определенного признака, при этом некоторое количество нейронов организовано в слои.
Несколько слоев нейронов могут называться полноценной НС.

Сама по себе сеть без обучения не может хорошо решать задачи. Для этого ей требуются две выборки – обучающая и проверочная.
Используя их, НС узнает, как правильно выполнить задачу. В этом ей помогает функция ошибки, которая указывает, в правильную ли сторону НС меняет свои веса.
После этого вы познакомились с полносвязным слоем, нейроны которого связаны со всеми входными нейронами.

Во второй части вы попробовали создать свою первую модель нейронной сети.
Подгрузили в ноутбук основу Sequential, которая отвечает за построение модели, и создали начальную модель: model = Sequential().
Но она выглядела как пустая коробка, в которую необходимо что-то положить. И первым стал слой Dense (полносвязный слой).
Так появилась модель, но ее еще нужно было обучить, и для этого вы указали оптимизатор и функцию потерь при помощи метода .compile().
Завершили все это вызовом метода .summary(), посмотрев, как выглядит структура НС.

Затем вы перешли к практике и решили первую задачу по распознаванию рукописных цифр.
Для этого импортировали все необходимые инструменты и загрузили данные MNIST.
Определили форму массива данных и представление данных в виде картинки.
Преобразовали данные для модели НС, превратив картинку 28x28 пикселов в последовательность из 784 чисел.
Не оставили без внимания и метки классов. Чтобы сеть лучше классифицировала, перевели метки в формат one hot encoding.
По изученной схеме создали НС и приступили к ее обучению.
В завершение проверили, как обученная НС распознает отдельные изображения рукописных цифр из набора, на котором сеть не обучалась.

Итак, вы вспомнили все, что прошли в этом ноутбукe.

Поздравляем с первым шагом на пути изучения и применения нейронных сетей!
"""





