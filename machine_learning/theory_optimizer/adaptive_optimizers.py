"""
Адаптивные оптимизаторы

Цель адаптивных алгоритмов - отдельный learning rate для каждого из параметров.
Чем чаще и сильнее меняется параметр, тем меньше его следующие изменения.
"""

"""
1. Среднеквадратичное распространение корня Adagrad


Например, какой-то из нейронов в каждой итерации немного изменяет свои значения(0,0.1,0,0.1,0.2) и есть другой, который колеблется от 0 до 10, к примеру.
Эти скачки нужно сгладить. Для этого мы по каждому нейрону храним его историю.

Это дает нам возможность рассчитать размер следующий шаг как корень из произведения квадратов его двух предыдущих шагов.
Таким образом, чем больше были предыдущие шаги изменений для нейрона, тем меньше буду следующие, это помогает "успокоить" особо активные нейроны.

Но существует вероятность застрять в локальном минимуме, потому что скорость постоянно падает. 
"""

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(64, input_shape=(10,), activation='softmax'))
adagrad = optimizers.Adagrad(learning_rate=0.01)
model.compile(loss='mean_squared_error', optimizer=adagrad)




"""
2. Экспоненциально затухающее среднее значение (RMSprop, root mean square)

Существенным свойством RMSprop является то, что вы не ограничены только суммой прошлых градиентов, но вы более ограничены градиентами последних временных шагов.
RMSprop вносит свой вклад в экспоненциально затухающее среднее значение прошлых «квадратичных градиентов».

В RMSProp мы пытаемся уменьшить вертикальное движение, используя среднее значение.
Появляется новая зависимость размера шага от предыдущих.
Уходит обязательность того, если 10 шагов назад был большой шаг, то сейчас должен быть меньший, и это позволяет выбраться из ямы.
То есть он немного буксует в локальном минимуме, а потом снова начинает увеличиваться при необходимости, потому что зависимость от предыдущих шагов экспоненциальная.

Чаще всего используется в генеративных алгоритмах.
"""

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(64, input_shape=(10,), activation='softmax'))
rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)
model.compile(loss='mean_squared_error', optimizer=rmsprop)