import tensorflow as tf
# Импорт класса создания последовательной модели Sequential
from tensorflow.python.keras.models import Sequential


# Создание экземпляра модели
model = Sequential()

# Настраиваем сеть
# input_dim - сеть принимает на вход последовательность из 10 чисел
# Dense - Линейный/полносвязный слой
model.add(tf.keras.layers.Dense(32, input_dim=10))

# Теперь нужно сеть скомпилировать (собрать, подготовить к обучению) и обучить.
# Для подготовки к обучению понадобятся две вещи: 1. оптимизатор 2. функция потерь (или функция ошибки)

# Функция потерь:
# 1. Кросс-энтропия (log) categorical_crossentropy
# 2. Среднеквадратичная ошибка (1/n*sum(предсказанные значения - прогнозируемые)^2) mean squared error MSE
# 3. Средняя абсолютная ошибка (1/n*sum(abs(предсказанные значения - прогнозируемые))) mean absolute error MAE
# 4. Бинарная кроссэнтропия (log) binary crossentropy BCE

# Оптимизаторы:
# SGD (стохастический градиентный спуск) Берем случайные объекты, подаем их в модель и получаем предсказание, считаем функцию потерь, обновляем веса, повторяем снова до тех пор пока функция ошибки не окажется в точке минимума
# Adagrad (среднеквадратичное распространение корня) Например, какой-то из нейронов в каждой итерации немного изменяет свои значения(0,0.1,0,0.1,0.2) и есть другой, который колеблется от 0 до 10, к примеру. Эти скачки нужно сгладить. Для этого мы по каждому нейрону храним его историю, это дает нам возможность рассчитать размер следующий шаг как корень из произведения квадратов его двух предыдущих шагов. Таким образом, чем больше были предыдущие шаги изменений для нейрона, тем меньше буду следующие, это помогает "успокоить" особо активные нейроны.
# RMSprop (экспоненциально затухающее среднее) В RMSProp мы пытаемся уменьшить вертикальное движение, используя среднее значение.
# Adam Чем больше движемся в одну сторону, тем больше шаг и адаптация каждого параметра. Алгоритм вычисляет экспоненциальное скользящее среднее градиента и квадратичный градиент


# Они задаются с помощью метода модели .compile()
model.compile(loss='categorical_crossentropy',
              optimizer='adam')

# Посмотрим, как теперь выглядит сеть, вызывая метод .summary():
model.summary()

# parameters = 352 (32*10 + 32 дополнительных нейрона)
# Доп. нейроны = нейроны смещения (чтобы иметь возможность сместить область определения) Бывают ситуации, в которых нейросеть просто не сможет найти верное решение из-за того, что нужная точка будет находиться вне пределов досягаемости



