"""
Функции активации

Определяют выходное значение нейрона в зависимости от результата взвешенной суммы входов и порогового значения.
Применяются к нейрону после того, просчитанному матричным умножением.
Определяет, передавать ли полученное значение следующему нейрону (активировать его) или нет, и модифицировать ли как-то полученное значение.



Рассмотрим нейрон, у которого взвешенная сумма равна z.

z = Σ (wi * xi + b)

wi - вес
xi - входное значение i-ого входа
b - смещение

Полученный результат передается в функцию активации, которая решает рассматривать этот нейрон как активированный, или его можно игнорировать.
"""


# Функция ReLU (Rectified Linear Unit) — это наиболее часто используемая функция активации при глубоком обучении.
# Данная функция возвращает 0, если принимает отрицательный аргумент, в случае же положительного аргумента, функция возвращает само число.
# То есть она может быть записана как f(z)=max(0,z).



# ReLU - возвращает входные значения без изменения, если знак значений положительный; отрицательные значения преобразует к нулю.
# Вычисляется по программной формуле (Python): relu = max(x, 0)

# ReLU даёт хорошие результаты точности в полносвязных слоях (Dense layers) и свёрточных слоях (Convolutional layers)